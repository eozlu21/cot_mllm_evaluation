Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.12s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.02s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.10it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.55it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.28it/s]
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:  50%|█████     | 1/2 [25:55<25:55, 1555.30s/it]Fetching 2 files:  50%|█████     | 1/2 [25:55<25:55, 1555.30s/it]
Traceback (most recent call last):
  File "/scratch/users/eozlu21/cot_mllm_evaluation/main.py", line 54, in <module>
    main()
  File "/scratch/users/eozlu21/cot_mllm_evaluation/main.py", line 38, in main
    verifier = LLMVerifier(args.judge_model)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 5, in __init__
  File "/scratch/users/eozlu21/cot_mllm_evaluation/cot_mllm_evaluation/verifier/huggingface.py", line 18, in __post_init__
    self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/users/eozlu21/micromamba/envs/mllm_verifier_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/users/eozlu21/micromamba/envs/mllm_verifier_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/users/eozlu21/micromamba/envs/mllm_verifier_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4260, in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/users/eozlu21/micromamba/envs/mllm_verifier_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 1152, in _get_resolved_checkpoint_files
    checkpoint_files, sharded_metadata = get_checkpoint_shard_files(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/users/eozlu21/micromamba/envs/mllm_verifier_env/lib/python3.11/site-packages/transformers/utils/hub.py", line 1115, in get_checkpoint_shard_files
    cached_filenames = cached_files(
                       ^^^^^^^^^^^^^
  File "/scratch/users/eozlu21/micromamba/envs/mllm_verifier_env/lib/python3.11/site-packages/transformers/utils/hub.py", line 517, in cached_files
    raise EnvironmentError(
OSError: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B does not appear to have a file named model-00002-of-000002.safetensors. Checkout 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/tree/main'for available files.
